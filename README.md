# simple-crawler


Данный проект загружает информацию с сайта lostfilm.tv и заносит в БД. Состоит из двух частей: скрипта загрузки и страницы просмотра.

## Скрипт загрузки 
устанавливается в крон, раз в день ( я бы установил в 00:00 )
	
	[/usr/www/crawler/]artisan command:parse [глубина просмотра]
	
Указывается полный путь до скрипта/команды artisan

После запуска, скрипт загружает в БД определенное в командной строке, кол-во страниц.  Сам парсер написан на коленке, по этому мы смотрим куда нам можно двигаться дальше.

## Куда двигаться дальше

1. *Загрузка страниц* - использовать multi_curl или найти подходящий компонент. Можно использовать Grizzle, по анализу анализу HTTP заголовков включать парсер или нет. Могут быль ошибки: 500, 403, 404, ... Писать загрузочные логи. 

2. *Парсинг страниц* - Для парсинга я бы использовал WebKit, есть реализация на Python, NodeJS, Scala и др. Имел опыт с первыми двумя. На Python больше возможностей. Как вариант, возможен парсинг страниц по средством DOMXML, однако, часто, бывают не валидные страницы и тут как повезет..

Еще можно нормализовать БД, сделать две таблицы Films, Episodes.

## Что еще
В качестве глобальной переменной, используется кеш, где хранится последний скачаный фильм. Следующая загрузка будет осуществляться вплоть до этого фильма, но не более указанной глубины просмотра страниц.


## Скрипт просмотра 

Просмотр осуществляется на домашней страницы http://website.com/ (http://127.0.0.1:8000)
Кол-во элементов вывода на странице проеделено в конфиге .env:
	
	VIEW_PAGING=8

или по умолчанию 10



## Развертывание

1. устанавливаем composer

2. скачиваем из репоситория 
	
	git clone https://github.com/akalend/simple-crawler.git

3. запускаем менеджер пакетов
	
	composer install

4. звпускаем миграцию

	./artisan 	migrate

6. настраиваем крон от пользователя (crontab -e)

	
	1 0  *   *   *  [/usr/www/crawler/]artisan command:parse [глубина просмотра]

7. пользуемся
