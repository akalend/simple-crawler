# simple-crawler


Данный проект загружает информацию с сайта lostfilm.tv и заносит в БД. Состоит из двух частей: скрипта загрузки и страницы просмотра.

## Скрипт загрузки устанавливается в крон, раз в день ( я бы установил в 00:00 )
	
	/usr/www/artisan command:parse [глубина просмотра]
	

После запуска, скрипт загружает в БД определенное в командной строке, кол-во страниц.  Сам парсер написан на коленке, по этому мы смотрим куда нам можно двигаться дальше.

## Куда двигаться дальш

1. *Загрузка страниц* - использовать multi_curl или найти подходящий компонент. Можно использовать Grizzle, по анализу анализу HTTP заголовков включать парсер или нет. Могут быль ошибки: 500, 403, 404, ... Писать загрузочные логи. 

2. *Парсинг страниц* - Для парсинга я бы использовал WebKit, есть реализация на Python, NodeJS, Scala и др. Имел опыт с первыми двумя. На Python больше возможностей. Как вариант, возможен парсинг страниц по средством DOMXML, однако, часто, бывают не валидные страницы и тут как повезет..


## Скрипт просмотра 

Кол-во элементов вывода на странице проеделено в конфиге view.php :
	
	'paging' => 7

можно определить, через .env



## Развертывание


